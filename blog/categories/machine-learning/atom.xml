<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Into Data]]></title>
  <link href="http://haipengliu.github.io/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://haipengliu.github.io/"/>
  <updated>2014-12-23T21:14:59+08:00</updated>
  <id>http://haipengliu.github.io/</id>
  <author>
    <name><![CDATA[Haipeng Liu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Introduction to CRF(2)]]></title>
    <link href="http://haipengliu.github.io/blog/2014/11/18/introduction-to-crf-2/"/>
    <updated>2014-11-18T17:27:47+08:00</updated>
    <id>http://haipengliu.github.io/blog/2014/11/18/introduction-to-crf-2</id>
    <content type="html"><![CDATA[<p>Author: Haipeng Liu <A href="mailto:pliu1981@gmail.com">email to me</A></br>
keywords: CRF</br></p>

<h1>Feature Engineering</h1>

<p>Features are perhaps the most important elements of the statistic model. Good feature engineering can often increase the labelling accuracy very significantly.</p>

<p>A feature function associates the labels and the local data patterns. The node feature take into account the current label $x_t$ and an element of data pattern $g_m[z, t]$. where $\delta[x_t, s]$ is the indicator function. It returns 1 if label of $x$ at $t$ is the label $s$ and 0 otherwise.</p>

<p>The data pattern function $g_m(z, t)$ takes care of the raw data and returns a (real or binary) value. For example, in noun-phrase chunking, we may have a feature $g$ the 100th.  That is, the 100th data feature receives value of 1 at current time $t$ of $10$ if the previous word is <em>today</em>.</p>

<p>&lt;&ndash;! more &ndash;></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to CRF(1)]]></title>
    <link href="http://haipengliu.github.io/blog/2014/11/15/my-first-blog/"/>
    <updated>2014-11-15T08:55:38+08:00</updated>
    <id>http://haipengliu.github.io/blog/2014/11/15/my-first-blog</id>
    <content type="html"><![CDATA[<p>Author: Haipeng Liu <A href="mailto:pliu1981@gmail.com">email to me</A></br>
keywords: CRF</br></p>

<h1>Sequential labelling problem</h1>

<p><strong>Conditional random</strong> field is proposed to model the sequence data. The sequence data are the data or objects with a sequence structure.
For example, in Noun-Phrase (NP) chunking task, for a sentence, we want to provide correct phrasal labels for each word. The phrasal labels are B-NP for the beginning words of a noun-phrase, I-NP for the words inside a noun-phase and O for others. For instant, a labelled sentence in the CoNLL2000 dataset reads like this.</p>

<blockquote><p>Chancellor/O of/O the/B-NP Exchequer/I-NP Nigel/B-NP Lawson/I-NP &rsquo;s/B-NP restated/I-NP commitment/I-NP to/O a/B-NP firm/I-NP monetary/I-NP policy/I-NP has/O helped/O to/O prevent/O a/B-NP freefall/I-NP in/O sterling/B-NP over/O the/B-NP past/I-NP week/I-NP ./O</p></blockquote>

<p><em>CoNLL2000</em></p>

<p>The CRF can automatically label the noun phrase character as the human does</p>

<h1>Model the sequence</h1>

<p>To model the labeling process. We use $x$ to denote the labels and $z$ to denote the data. In our case $x$ is the sequence of noun phrase labels and $z$ is the sequence of words. $t$ is the sequence index. Also, $x$ is named hidden state variables, $z$ is named the observations. The task of CRF is to predict the state variables given the observations. That is to compute the probability:
   <!-- more --> <br/>
$P(x|z)$, where $x = (x_1, x_2, &hellip;, x_T)$ the labels, $z$ the data, $t\in[1, T]$ sequence index.</p>

<h1>Potential functions</h1>

<p>With the theory of undirected Markov chain, the conditional probability of $x$ given $z$ is defined as the product of a serial potential functions. Where:</br></p>

<p>$$P(x|z) = \frac{1}{Z(z)}\sum_{c}\psi_c(x_c, z)$$
$\psi_c(x_c, z)$ - potential function&lt;\br>
$Z(z) = \sum_x\prod_c\psi_c(x_c, z)$ - partition function&lt;\br></p>

<p> $\psi$ are positive functions known as potentials.</br>
 $Z(z)$ is the normalization constant known as the partition function, which guarantees the formula return a value in the range from zero to one. It’s the sum of the potentials but accounting for all the possible x values, which can be treated as the edge probabilities along side the $x$. </br>
 The number of potentials is denoted by $c$. Here in undirected Markov chain, $c$ stands for clique, which means different connections among nodes. In CRF, generally there are two kinds of connections, one is the current node connecting observation nodes, another is the nearby two nodes connecting observations nodes. They are corresponded to two kinds of potentials – <strong>node potential</strong> and <strong>edge potential</strong>.</p>

<blockquote><p>Two types of potentials</br>
node potential of label $x_t$ at time $t$</p></blockquote>

<p>$$\begin{equation}\phi_t(x_t, z) = exp(\sum_k\omega_kf_k(x_t, z, t))\dots\end{equation}~Eq.(1)\dots~a~S\times{}T~matrix$$</p>

<blockquote><p>edge potential of two nearby labels at time $t$ and $t+1$</p></blockquote>

<p>$$\begin{equation}\varphi_t(x_t, x_t+1, z) = exp(\sum_k\omega_kf_k(x_t, x_t+1, z_t))\dots\end{equation}~Eq.(2)\dots~a~S\times{}S\times{}T~matrix$$</p>

<p>The concept of potentials can be imagined as energies. Those local, nearby pieces of energies are used to compose the probability. The potentials refer to some quantities that tie the labels and the local data patterns at particular time together. In the simplest case which is often used in practice, potentials are exponential of sum of weighted features. There are usually two types of potential functions: node potential functions and edge potential functions, according to different clique styles they are connected.</p>

<p>The node potential $\phi$ describes the contribution of the current node and its connecting observations to the global model. The edge potential $\varphi_t$ describes the contribution of two nearby labels and their connecting observations to the global model. The connecting styles between nodes and observations are modeled by feature functions. Where $f_k$ is called the feature function, $\omega_k$ is the weight for a feature and $k$ is the number of features. Feature functions connect the labels and the observed data. There may be S labels and T observations. So each potential function is a multi-dimensionl matrix.</p>

<p>The node potential can be represented by a $S\times{}T$ matrix ($S$ stands for the number of labels and $T$ stands for the number of input items. The edge potential can be implemented as an $S\times{}S\times{}T$ matrix.
Since we have two type of potentials, then we can get two type of features.</p>
]]></content>
  </entry>
  
</feed>
